{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "import names\n",
    "import gender_guesser.detector as gender\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Band Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titles', 'rb') as inf:\n",
    "    titles = pickle.load(inf)\n",
    "    \n",
    "determiners = []\n",
    "nouns = []\n",
    "adjectives = []\n",
    "for title in titles:\n",
    "    tagged_title = nltk.pos_tag(nltk.word_tokenize(title.lower()))\n",
    "    for tagged_word in tagged_title:\n",
    "        word = tagged_word[0]\n",
    "        pos = tagged_word[1]\n",
    "        if pos == 'DT':\n",
    "            determiners.append(word)\n",
    "        elif pos == 'NN' or pos == 'NNS':\n",
    "            nouns.append(word.capitalize())\n",
    "        elif pos == 'JJ':\n",
    "            adjectives.append(word.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_band_name(text):\n",
    "    def getRandName():\n",
    "        determiner = random.choice(determiners).capitalize()\n",
    "        [adjective1, adjective2] = random.sample(adjectives, 2)\n",
    "        [noun1, noun2] = random.sample(nouns, 2)\n",
    "\n",
    "        title_format = random.randrange(3)\n",
    "        if title_format == 0:\n",
    "            return determiner + ' ' + adjective1 + ' ' + noun1\n",
    "        elif title_format == 1:\n",
    "            return determiner + ' ' + adjective1 + ' ' + adjective2 + ' ' + noun1\n",
    "        elif title_format == 2:\n",
    "            return determiner + ' ' + noun1 + ' and ' + determiner + ' ' + noun2\n",
    "        \n",
    "    new_name = getRandName()\n",
    "    text = re.sub('\\[BAND_NAME\\]', new_name, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_years(text):\n",
    "    num_years = text.count('[YEAR]')\n",
    "    first_year = datetime.now().year - (5 * num_years)\n",
    "    years = [first_year]\n",
    "    for i in range(1, num_years):\n",
    "        years.append(years[i-1] + random.randint(0, 5))\n",
    "        \n",
    "    j = -1\n",
    "    def get_year(matchobj):\n",
    "        nonlocal j\n",
    "        j += 1\n",
    "        return years[j]\n",
    "\n",
    "    return re.sub(\"\\[YEAR\\]\", lambda x: str(get_year(x)), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_months(text):\n",
    "    months = [calendar.month_name[i] for i in range(1,13)] + [calendar.month_abbr[i] for i in range(1,13)]\n",
    "    return re.sub('\\[MONTH\\]', lambda x: random.choice(months), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Names (doesn't work currently)\n",
    "https://stackoverflow.com/questions/20290870/improving-the-extraction-of-human-names-with-nltk\n",
    "Shivansh bhandari's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stefanie Sargent', 'Valerie Agnew', 'Ben London', \"Sick 'Em\", 'Roisin Dunne', 'Home Alive', 'Miami Beach']\n"
     ]
    }
   ],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)\n",
    "\n",
    "names_ = get_human_names(text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: function to replace names with autogen names\n",
    "maybe create dict that matches old to new\n",
    "also need to replace last names as well as first names (maybe first names too?)\n",
    "i think can replace all full names, then can look for last or first because shouldn't be an issue that we already replaced some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stefanie Sargent female\n",
      "Valerie Agnew female\n",
      "Ben London male\n",
      "Sick 'Em unknown\n",
      "Roisin Dunne female\n",
      "Home Alive unknown\n",
      "Miami Beach unknown\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Stefanie Sargent': 'female',\n",
       " 'Valerie Agnew': 'female',\n",
       " 'Ben London': 'male',\n",
       " 'Roisin Dunne': 'female'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_genders = {}\n",
    "d = gender.Detector()\n",
    "for name in person_names:\n",
    "    g = d.get_gender(name.split()[0])\n",
    "    print(name, g)\n",
    "    if g == 'male' or g == 'female':\n",
    "        name_genders[name] = g\n",
    "name_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Stefanie Sargent': 'Danielle Grady',\n",
       " 'Valerie Agnew': 'Lucy Nelson',\n",
       " 'Ben London': 'David Gambino',\n",
       " 'Roisin Dunne': 'Emily Dancy'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_replacements = {}\n",
    "for k,v in name_genders.items():\n",
    "    name_replacements[k] = names.get_full_name(gender=v)\n",
    "name_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in name_replacements.items():\n",
    "    text = re.sub(k.split()[0], v.split()[0], text) # replace first name\n",
    "    text = re.sub(k.split()[1], v.split()[1], text) # replace last name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/consolidatedData.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "genres = set()\n",
    "for genre in data['allGenres']:\n",
    "    if isinstance(genre, str):\n",
    "        g = re.split(',|\\[', genre)[0]\n",
    "        genres.add(g)\n",
    "        \n",
    "genres = list(genres) # need to convert to list to be able to take a random choice from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_genre(text):\n",
    "    return re.sub('\\[GENRE\\]', random.choice(genres), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace All Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAND_NAME] is the third studio EP by the American rock band Alice in Chains, released on [MONTH] 25, [YEAR], through Columbia Records. This is Alice in Chains' second acoustic EP, preceded by [YEAR]'s Sap, and it is the first EP in music history to debut at No. 1 on the Billboard 200 chart, with the first week sales exceeding 141,000 copies in the United States. The self-produced EP was written and recorded over the course of just one week at the London Bridge Studio in Seattle. The tracks \"No Excuses\", \"I Stay Away\" and \"Don't Follow\" were released as singles to promote the album. [BAND_NAME] was nominated for two Grammy Awards in [YEAR]; Best Recording Package and Best Hard Rock Performance for \"I Stay Away\".\n",
      "The EP was well received by critics and has been certified triple-platinum by the RIAA, selling 4 million copies worldwide, making [BAND_NAME] one of the band's most successful releases. In Canada, [BAND_NAME] was certified double-platinum for the sale of 200,000 copies. In Great Britain, the album was certified silver after selling 60,000 copies.\n",
      "Following Alice in Chains' extensive [YEAR] world tour for Dirt, bassist Mike Starr getting fired during the tour for his drug use, and [PERSON_NAME_FULL_0_MALE] bassist Mike Inez joining the band, the members returned home to Seattle after the end of their Lollapalooza tour and found themselves evicted from their residence after failing to pay the rent. Homeless, the band then moved into the London Bridge Studio in Seattle.\n",
      "[MONTH]\n",
      "The Dear Mu is the third studio EP by the American rock band Alice in Chains, released on Jan 25, 2001, through Columbia Records. This is Alice in Chains' second acoustic EP, preceded by 2005's Sap, and it is the first EP in music history to debut at No. 1 on the Billboard 200 chart, with the first week sales exceeding 141,000 copies in the United States. The self-produced EP was written and recorded over the course of just one week at the London Bridge Studio in Seattle. The tracks \"No Excuses\", \"I Stay Away\" and \"Don't Follow\" were released as singles to promote the album. The Dear Mu was nominated for two Grammy Awards in 2008; Best Recording Package and Best Hard Rock Performance for \"I Stay Away\".\n",
      "The EP was well received by critics and has been certified triple-platinum by the RIAA, selling 4 million copies worldwide, making The Dear Mu one of the band's most successful releases. In Canada, The Dear Mu was certified double-platinum for the sale of 200,000 copies. In Great Britain, the album was certified silver after selling 60,000 copies.\n",
      "Following Alice in Chains' extensive 2012 world tour for Dirt, bassist Mike Starr getting fired during the tour for his drug use, and [PERSON_NAME_FULL_0_MALE] bassist Mike Inez joining the band, the members returned home to Seattle after the end of their Lollapalooza tour and found themselves evicted from their residence after failing to pay the rent. Homeless, the band then moved into the London Bridge Studio in Seattle.\n",
      "May\n"
     ]
    }
   ],
   "source": [
    "with open('data/consolidatedData.json', 'r', encoding='utf-8') as inf:\n",
    "    paras = json.load(inf)\n",
    "\n",
    "#with open('data/0.json', 'r', encoding='utf-8') as inf:\n",
    "#    data = json.load(inf)\n",
    "    \n",
    "para = paras['allPrunedParaComplete'][1]\n",
    "para += '[MONTH]'\n",
    "print(para)\n",
    "para = replace_band_name(para)\n",
    "para = replace_years(para)\n",
    "para = replace_months(para)\n",
    "para = replace_genre(para)\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
