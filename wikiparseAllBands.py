# -*- coding: utf-8 -*-
"""WikiParser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f6i-l5fDdR-PpsA-2FLRNnsQGN5ChxPV
"""

import requests
import urllib.request
import time
from bs4 import BeautifulSoup
import re
from urllib.request import urlopen
import json
import pandas
from parse_infobox import parse_infobox


def extractRawData(soup):
  """Inserts text into overall dictionary skeleton"""

  bandData["Name"] = soup.title.string[:-12] # get band name


  mydiv = soup.find("div", {"class": "mw-parser-output"})
  children = mydiv.findChildren()

  rawData = []
  numParagraphs = 3 # We only care about the first 2-3 paragraphs
  for c in children:
    if c.name == 'p':
      if numParagraphs > 0:
        if (c.getText() != "\n"):
          rawData.append(re.sub("[\[].*?[\]]", "", c.getText()))
          numParagraphs-=1
  bandData["rawData"] = rawData

def createJSONData(url):
  """Create JSON object representing wikipedia article"""
  html = urlopen(url) 
  soup = BeautifulSoup(html, 'html.parser')

  extractRawData(soup)
  bandData["infobox"] = parse_infobox(soup)
  allBandNames.append(bandData["Name"])


##########################################################################

colnames = ['url']
data = pandas.read_csv('list_of_bands.csv', names=colnames)
urls = data.url.tolist()

allBandNames = []
fileNum = 0
for url in urls[0:4]:
  bandData = {}
  createJSONData(url)
  
  with open('./data/'+str(fileNum)+'.json', 'w', encoding="utf-8") as outfile:
     json.dump(bandData, outfile, indent = 3,
               ensure_ascii = False)

  fileNum = fileNum + 1

print(allBandNames)